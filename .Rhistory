set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(10)
# The parameter of interest: a "location parameter" (in this exercise, this should be thought as "axis of symmetry for an unknown density function"
# This number is NOT known by the statistician
trueLocationParameter <- 1.5
# This is our version of nature: a function simulating a dataset of size 1000
generateDataset <- function() {
# You will not need to know the details of how the data is generated
# Other than that it is controlled by a symmetric density around the parameter trueLocationParameter
# And that there is a second parameter which controls how 'noisy' the data will be.
return(rt(nObservations, dataGenerationSetup, trueLocationParameter))
}
# An example of an estimator
sampleAverage <- function(observations) {
return(mean(observations))
# PS: key point here is that the unknown parameter trueLocationParameter was not used in this function!
}
# Another estimator we wish to compare
sampleMedian <- function(observations) {
return(median(observations))
# To be completed
}
# Another estimator we wish to compare
trimmedMean <- function(observations) {
return(mean(observations, trim = 0.1))
# To be completed
}
# Each dataset will contain that many observations:
nObservations <- 1000
# The function below uses the idea at the core of frequentist thinking: what if we performed the same "experiment" many times?
# Specifically, here we use this idea to approximate the MSE of an estimator: as we repeat an experiment (i.e. generate datasets) and apply Alice's
# estimator to each dataset, how far from the truth would the estimator be in average?
#
# The input argument ESTIMATOR is a function that takes a dataset as input and tries to guess the unknown parameter from it.
# Here we will consider the sampleAverage and the trimmedMean
approximateMSE <- function(ESTIMATOR, numberOfExperiments) {
sumErrorSquared <- 0
for (i in 1:numberOfExperiments) {
data <- generateDataset()
estimate <- ESTIMATOR(data)
plot(data)
print(data < 1.5)
print(data > 1.5)
sumErrorSquared <- sumErrorSquared + (estimate - trueLocationParameter)^2
}
return(sumErrorSquared / numberOfExperiments)
}
# Barbara uses approximateMSE to evaluate different estimators
compareEstimators <- function(numberOfExperiments) {
cat("  MSE of the sampleAverage estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleAverage, numberOfExperiments), "\n")
# Uncomment the lines below once you have implemented the estimators
cat("  MSE of the sampleMedian estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleMedian, numberOfExperiments), "\n")
cat("  MSE of the trimmedMean estimator (", numberOfExperiments," experiments): ", approximateMSE(trimmedMean, numberOfExperiments), "\n")
}
# Good habit: make your code reproducible by fixing random seed (this is like picking an omega in a probability space)
set.seed(123)
# We will consider 3 types of dataset, one where the parameter is easy to estimate, one where it is harder (noisier), and a third one where it is crazy hard
easy <- 100
noisy <- 2
crazy <- 0.99
setDatasetDifficulty <- function(difficulty) {
# sets a global variables (typically not good practice but does the job in this exercise)
dataGenerationSetup <<- difficulty
}
cat("Approximations (based on LLN, also known as a Monte Carlo approximation) for...\n")
set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(10)
# The parameter of interest: a "location parameter" (in this exercise, this should be thought as "axis of symmetry for an unknown density function"
# This number is NOT known by the statistician
trueLocationParameter <- 1.5
# This is our version of nature: a function simulating a dataset of size 1000
generateDataset <- function() {
# You will not need to know the details of how the data is generated
# Other than that it is controlled by a symmetric density around the parameter trueLocationParameter
# And that there is a second parameter which controls how 'noisy' the data will be.
return(rt(nObservations, dataGenerationSetup, trueLocationParameter))
}
# An example of an estimator
sampleAverage <- function(observations) {
return(mean(observations))
# PS: key point here is that the unknown parameter trueLocationParameter was not used in this function!
}
# Another estimator we wish to compare
sampleMedian <- function(observations) {
return(median(observations))
# To be completed
}
# Another estimator we wish to compare
trimmedMean <- function(observations) {
return(mean(observations, trim = 0.1))
# To be completed
}
# Each dataset will contain that many observations:
nObservations <- 1000
# The function below uses the idea at the core of frequentist thinking: what if we performed the same "experiment" many times?
# Specifically, here we use this idea to approximate the MSE of an estimator: as we repeat an experiment (i.e. generate datasets) and apply Alice's
# estimator to each dataset, how far from the truth would the estimator be in average?
#
# The input argument ESTIMATOR is a function that takes a dataset as input and tries to guess the unknown parameter from it.
# Here we will consider the sampleAverage and the trimmedMean
approximateMSE <- function(ESTIMATOR, numberOfExperiments) {
sumErrorSquared <- 0
for (i in 1:numberOfExperiments) {
data <- generateDataset()
estimate <- ESTIMATOR(data)
plot(data)
print((data < 1.5) == TRUE)
sumErrorSquared <- sumErrorSquared + (estimate - trueLocationParameter)^2
}
return(sumErrorSquared / numberOfExperiments)
}
# Barbara uses approximateMSE to evaluate different estimators
compareEstimators <- function(numberOfExperiments) {
cat("  MSE of the sampleAverage estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleAverage, numberOfExperiments), "\n")
# Uncomment the lines below once you have implemented the estimators
cat("  MSE of the sampleMedian estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleMedian, numberOfExperiments), "\n")
cat("  MSE of the trimmedMean estimator (", numberOfExperiments," experiments): ", approximateMSE(trimmedMean, numberOfExperiments), "\n")
}
# Good habit: make your code reproducible by fixing random seed (this is like picking an omega in a probability space)
set.seed(123)
# We will consider 3 types of dataset, one where the parameter is easy to estimate, one where it is harder (noisier), and a third one where it is crazy hard
easy <- 100
noisy <- 2
crazy <- 0.99
setDatasetDifficulty <- function(difficulty) {
# sets a global variables (typically not good practice but does the job in this exercise)
dataGenerationSetup <<- difficulty
}
cat("Approximations (based on LLN, also known as a Monte Carlo approximation) for...\n")
set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(10)
# The parameter of interest: a "location parameter" (in this exercise, this should be thought as "axis of symmetry for an unknown density function"
# This number is NOT known by the statistician
trueLocationParameter <- 1.5
# This is our version of nature: a function simulating a dataset of size 1000
generateDataset <- function() {
# You will not need to know the details of how the data is generated
# Other than that it is controlled by a symmetric density around the parameter trueLocationParameter
# And that there is a second parameter which controls how 'noisy' the data will be.
return(rt(nObservations, dataGenerationSetup, trueLocationParameter))
}
# An example of an estimator
sampleAverage <- function(observations) {
return(mean(observations))
# PS: key point here is that the unknown parameter trueLocationParameter was not used in this function!
}
# Another estimator we wish to compare
sampleMedian <- function(observations) {
return(median(observations))
# To be completed
}
# Another estimator we wish to compare
trimmedMean <- function(observations) {
return(mean(observations, trim = 0.1))
# To be completed
}
# Each dataset will contain that many observations:
nObservations <- 1000
# The function below uses the idea at the core of frequentist thinking: what if we performed the same "experiment" many times?
# Specifically, here we use this idea to approximate the MSE of an estimator: as we repeat an experiment (i.e. generate datasets) and apply Alice's
# estimator to each dataset, how far from the truth would the estimator be in average?
#
# The input argument ESTIMATOR is a function that takes a dataset as input and tries to guess the unknown parameter from it.
# Here we will consider the sampleAverage and the trimmedMean
approximateMSE <- function(ESTIMATOR, numberOfExperiments) {
sumErrorSquared <- 0
for (i in 1:numberOfExperiments) {
data <- generateDataset()
estimate <- ESTIMATOR(data)
plot(data)
print(length(data < 1.5) == TRUE)
sumErrorSquared <- sumErrorSquared + (estimate - trueLocationParameter)^2
}
return(sumErrorSquared / numberOfExperiments)
}
# Barbara uses approximateMSE to evaluate different estimators
compareEstimators <- function(numberOfExperiments) {
cat("  MSE of the sampleAverage estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleAverage, numberOfExperiments), "\n")
# Uncomment the lines below once you have implemented the estimators
cat("  MSE of the sampleMedian estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleMedian, numberOfExperiments), "\n")
cat("  MSE of the trimmedMean estimator (", numberOfExperiments," experiments): ", approximateMSE(trimmedMean, numberOfExperiments), "\n")
}
# Good habit: make your code reproducible by fixing random seed (this is like picking an omega in a probability space)
set.seed(123)
# We will consider 3 types of dataset, one where the parameter is easy to estimate, one where it is harder (noisier), and a third one where it is crazy hard
easy <- 100
noisy <- 2
crazy <- 0.99
setDatasetDifficulty <- function(difficulty) {
# sets a global variables (typically not good practice but does the job in this exercise)
dataGenerationSetup <<- difficulty
}
cat("Approximations (based on LLN, also known as a Monte Carlo approximation) for...\n")
set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(1)
# The parameter of interest: a "location parameter" (in this exercise, this should be thought as "axis of symmetry for an unknown density function"
# This number is NOT known by the statistician
trueLocationParameter <- 1.5
# This is our version of nature: a function simulating a dataset of size 1000
generateDataset <- function() {
# You will not need to know the details of how the data is generated
# Other than that it is controlled by a symmetric density around the parameter trueLocationParameter
# And that there is a second parameter which controls how 'noisy' the data will be.
return(rt(nObservations, dataGenerationSetup, trueLocationParameter))
}
# An example of an estimator
sampleAverage <- function(observations) {
return(mean(observations))
# PS: key point here is that the unknown parameter trueLocationParameter was not used in this function!
}
# Another estimator we wish to compare
sampleMedian <- function(observations) {
return(median(observations))
# To be completed
}
# Another estimator we wish to compare
trimmedMean <- function(observations) {
return(mean(observations, trim = 0.1))
# To be completed
}
# Each dataset will contain that many observations:
nObservations <- 1000
# The function below uses the idea at the core of frequentist thinking: what if we performed the same "experiment" many times?
# Specifically, here we use this idea to approximate the MSE of an estimator: as we repeat an experiment (i.e. generate datasets) and apply Alice's
# estimator to each dataset, how far from the truth would the estimator be in average?
#
# The input argument ESTIMATOR is a function that takes a dataset as input and tries to guess the unknown parameter from it.
# Here we will consider the sampleAverage and the trimmedMean
approximateMSE <- function(ESTIMATOR, numberOfExperiments) {
sumErrorSquared <- 0
for (i in 1:numberOfExperiments) {
data <- generateDataset()
estimate <- ESTIMATOR(data)
plot(data)
print(table(data>1.5))
sumErrorSquared <- sumErrorSquared + (estimate - trueLocationParameter)^2
}
return(sumErrorSquared / numberOfExperiments)
}
# Barbara uses approximateMSE to evaluate different estimators
compareEstimators <- function(numberOfExperiments) {
cat("  MSE of the sampleAverage estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleAverage, numberOfExperiments), "\n")
# Uncomment the lines below once you have implemented the estimators
cat("  MSE of the sampleMedian estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleMedian, numberOfExperiments), "\n")
cat("  MSE of the trimmedMean estimator (", numberOfExperiments," experiments): ", approximateMSE(trimmedMean, numberOfExperiments), "\n")
}
# Good habit: make your code reproducible by fixing random seed (this is like picking an omega in a probability space)
set.seed(123)
# We will consider 3 types of dataset, one where the parameter is easy to estimate, one where it is harder (noisier), and a third one where it is crazy hard
easy <- 100
noisy <- 2
crazy <- 0.99
setDatasetDifficulty <- function(difficulty) {
# sets a global variables (typically not good practice but does the job in this exercise)
dataGenerationSetup <<- difficulty
}
cat("Approximations (based on LLN, also known as a Monte Carlo approximation) for...\n")
set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(1)
set.seed(123)
setDatasetDifficulty(noisy)
compareEstimators(1)
set.seed(123)
setDatasetDifficulty(crazy)
compareEstimators(1)
?rt()
# The parameter of interest: a "location parameter" (in this exercise, this should be thought as "axis of symmetry for an unknown density function"
# This number is NOT known by the statistician
trueLocationParameter <- 1.5
# This is our version of nature: a function simulating a dataset of size 1000
generateDataset <- function() {
# You will not need to know the details of how the data is generated
# Other than that it is controlled by a symmetric density around the parameter trueLocationParameter
# And that there is a second parameter which controls how 'noisy' the data will be.
return(rt(nObservations, dataGenerationSetup, trueLocationParameter))
}
# An example of an estimator
sampleAverage <- function(observations) {
return(mean(observations))
# PS: key point here is that the unknown parameter trueLocationParameter was not used in this function!
}
# Another estimator we wish to compare
sampleMedian <- function(observations) {
return(median(observations))
# To be completed
}
# Another estimator we wish to compare
trimmedMean <- function(observations) {
return(mean(observations, trim = 0.1))
# To be completed
}
# Each dataset will contain that many observations:
nObservations <- 1000
# The function below uses the idea at the core of frequentist thinking: what if we performed the same "experiment" many times?
# Specifically, here we use this idea to approximate the MSE of an estimator: as we repeat an experiment (i.e. generate datasets) and apply Alice's
# estimator to each dataset, how far from the truth would the estimator be in average?
#
# The input argument ESTIMATOR is a function that takes a dataset as input and tries to guess the unknown parameter from it.
# Here we will consider the sampleAverage and the trimmedMean
approximateMSE <- function(ESTIMATOR, numberOfExperiments) {
sumErrorSquared <- 0
for (i in 1:numberOfExperiments) {
data <- generateDataset()
estimate <- ESTIMATOR(data)
plot(data)
print(table(data>1.5))
print(estimate)
sumErrorSquared <- sumErrorSquared + (estimate - trueLocationParameter)^2
}
return(sumErrorSquared / numberOfExperiments)
}
# Barbara uses approximateMSE to evaluate different estimators
compareEstimators <- function(numberOfExperiments) {
cat("  MSE of the sampleAverage estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleAverage, numberOfExperiments), "\n")
# Uncomment the lines below once you have implemented the estimators
cat("  MSE of the sampleMedian estimator (", numberOfExperiments," experiments): ", approximateMSE(sampleMedian, numberOfExperiments), "\n")
cat("  MSE of the trimmedMean estimator (", numberOfExperiments," experiments): ", approximateMSE(trimmedMean, numberOfExperiments), "\n")
}
# Good habit: make your code reproducible by fixing random seed (this is like picking an omega in a probability space)
set.seed(123)
# We will consider 3 types of dataset, one where the parameter is easy to estimate, one where it is harder (noisier), and a third one where it is crazy hard
easy <- 100
noisy <- 2
crazy <- 0.99
setDatasetDifficulty <- function(difficulty) {
# sets a global variables (typically not good practice but does the job in this exercise)
dataGenerationSetup <<- difficulty
}
cat("Approximations (based on LLN, also known as a Monte Carlo approximation) for...\n")
set.seed(123)
setDatasetDifficulty(easy)
compareEstimators(1)
t<- qt(0.004, 443)
-0.164/t
pf(3.92, 4, 443, lower.tail = FALSE)
y_est = b0 + b1*x
b1 = 0.67249
b0 = 212.688972
x_mean = 597.2
y_est = b0 + b1*x
b1 = 0.67249
b0 = 212.688972
x_mean = 597.2
y_est = b0 + b1*x_mean
sqrt(1-0.685)*98.6
sqrt(1-0.685^2)*98.6
b0 + b1*500
b0 + b1*500 + 1.96*res
res = sqrt(1-0.685^2)*98.6
b0 + b1*500 + 1.96*res
b0 + b1*500 - 1.96*res
-0.5405 + 1.972 * 0.0625
-0.5405 - 1.972 * 0.0625
3.5605 + 1.989 * 0.7137
3.5605 - 1.989 * 0.7137
-14.2293 + 3.5605*16 + 3.3682*sqrt(6) - 0.6717*8 + 0.8136*35
setwd("/Users/eylulaygun/Desktop/Year\ 5/STAT\ 306/Group\ project/BeijingRealEstateRegressionAnalysis" )
getwd()
library("dplyr")
setwd("/Users/eylulaygun/Desktop/Year\ 5/STAT\ 306/Group\ project/BeijingRealEstateRegressionAnalysis" )
getwd()
library("dplyr")
# PART1: Data cleaning and general preparation
#data <- read.csv("new.csv", header = TRUE, fileEncoding="latin1") # had to typeset encoding to eliminate type convert error
# data simple has DOM removed from it, as the majority of DOM values are na, which reduces dataset size enormously( cuts N in half)
data_simple <- read.csv("new copy.csv", header = TRUE, fileEncoding="latin1") # had to typeset encoding to eliminate type convert error
str(data_simple) # sanity check
# remove NA values
data_simple <- na.omit( data.frame(data_simple))
# some columns are char when they have to be integers (eg. livingRoom, drawingRoom, etc)
data_simple$livingRoom <- as.numeric(data_simple$livingRoom)
data_simple$bathRoom <- as.numeric(data_simple$bathRoom)
data_simple$constructionTime <- as.numeric(data_simple$constructionTime)
data_simple$drawingRoom <- as.numeric(data_simple$drawingRoom)
# typecast categorical variables as factors
data_simple$district <- as.factor(data_simple$district)
data_simple$buildingType <- as.factor(data_simple$buildingType)
data_simple$buildingStructure <- as.factor(data_simple$buildingStructure)
# typeset tradeTime to Date Type
data_simple$tradeTime <- as.Date(data_simple$tradeTime)
str(data_simple)
# CREATE training and testing data
N <- length(data_simple$id)
set.seed(2020)
all_indices = seq(1, N)
training_indices = sort(sample(1:N, N/2, replace = FALSE))
training_set = data_simple[training_indices,]
testing_indices =  sort(all_indices[!all_indices %in% training_indices]) # remove training indices from set
testing_set = data_simple[testing_indices,]
all.equal(sort(c(training_indices, testing_indices)), all_indices) # sanity check to ensure we separated tests correctly
# EXPLORE CORRELATION BETWEEN VARIABLES
plot(training_set$totalPrice, training_set$square*training_set$price) # perfectly linear relationship
cor(training_set$totalPrice, training_set$square*training_set$price)
cor(training_set$totalPrice, training_set$price)
# it wouldn't make sense to calculate pearson correlation between a categorical variable that looks like an "integer" with a continuous var such as totalPrice
# But I was curious and made a plot to see how house prices differ across districts in Beijing
plot(training_set$totalPrice, training_set$district)
# additional plots for other categorical variables
plot(training_set$totalPrice, training_set$buildingType)
plot(training_set$totalPrice, training_set$buildingStructure)
# correlation matrices
corrs_wrt_totalPrice <- cor(select_if(training_set, is.numeric), training_set$totalPrice)
corrs_all <- cor(select_if(training_set, is.numeric)) # correlation between all variables
#
# DATA VISUALIZATION
library(ggplot2)
p <- ggplot(data = training_set, aes(x = livingRoom, y = totalPrice))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=totalPrice), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Sample Across Sectors for Optimal Allocation")
p <- ggplot(data = training_set, aes(x = district, y = totalPrice))
p
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=totalPrice), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Sample Across Sectors for Optimal Allocation")
x
aggregate(training_set[, 4:4 ], list(training_set$district), mean)
mean_price_by_district <- aggregate(training_set[, 4:4 ], list(training_set$district), mean)
mean_price_by_district <- data.frame(distrcit_number = mean_price_by_district$Group.1, mean_price = mean_price_by_district$x)
mean_price_by_district <- aggregate(training_set[, 4:4 ], list(training_set$district), mean)
mean_price_by_district <- data.frame(distrcit_number = mean_price_by_district$Group.1, mean_price = mean_price_by_district$x)
p <- ggplot(data = mean_price_by_district, aes(x = distrcit_number, y = mean_price))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=mean_price), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean House Price Across Districts")
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = mean_price_by_district$x)
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = round( mean_price_by_district$x))
p <- ggplot(data = mean_price_by_district, aes(x = district_number, y = mean_price))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=mean_price), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean House Price Across Districts")
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = round( mean_price_by_district$x))
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = round( mean_price_by_district$x))
mean_price_by_district$x
mean_price_by_district <- aggregate(training_set[, 4:4 ], list(training_set$district), mean)
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = round( mean_price_by_district$x))
p <- ggplot(data = mean_price_by_district, aes(x = district_number, y = mean_price))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=mean_price), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean House Price Across Districts")
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=mean_price), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean House Price Across Districts")
# how are building types distributed across districs?
building_type_by_district <- aggregate(training_set[, 11:11 ], list(training_set$district), count)
# how are building types distributed across districs?
building_type_by_district <- aggregate(training_set[, 11:11 ], list(training_set$district))
# BASIC LINEAR MODELS
model1 <- lm(totalPrice ~ square + district)
# BASIC LINEAR MODELS
model1 <- lm(totalPrice ~ square + district, data = training_set)
summary(model1)
# BASIC LINEAR MODELS
training_set$district <- relevel(training_set$district, ref = 13)
model1 <- lm(totalPrice ~ square + district, data = training_set)
summary(model1)
model1 <- lm(totalPrice ~ square + district, data = training_set)
summary(model1)
# where is bedroom??
model_basic2 <- lm(totalPrice ~ square + district + bathroom + drawingRoom + livingRoom , data = training_set)
# where is bedroom??
model_basic2 <- lm(totalPrice ~ square + district + bathRoom + drawingRoom + livingRoom , data = training_set)
summary(model_basic2)
View(corrs_all)
View(corrs_wrt_totalPrice)
# where is bedroom??
model_basic2 <- lm(totalPrice ~ square + district + bathRoom + drawingRoom + livingRoom + subway , data = training_set)
summary(model_basic2)
# where is bedroom??
model_basic2 <- lm(totalPrice ~ square + district + bathRoom + drawingRoom + livingRoom + subway+ communityAverage, data = training_set)
summary(model_basic2)
plot(training_set$totalPrice, training_set$communityAverage)
# trying to understand the most expensive neighborhoods
mean_community_score_by_district <- aggregate(training_set[, 19:19 ], list(training_set$district), mean)
mean_community_score_by_district <- data.frame(district_number = mean_community_score_by_district$Group.1, community_avg = round( mean_community_score_by_district$x))
p <- ggplot(data = mean_price_by_district, aes(x = district_number, y = community_avg))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=community_avg), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean Community Score Across Districts")
# trying to understand the most expensive neighborhoods
mean_community_score_by_district <- aggregate(training_set[, 19:19 ], list(training_set$district), mean)
mean_community_score_by_district
mean_community_score_by_district$x
mean_community_score_by_district <- data.frame(district_number = mean_community_score_by_district$Group.1, community_avg = round( mean_community_score_by_district$x))
p <- ggplot(data = mean_community_score_by_district, aes(x = district_number, y = community_avg))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=community_avg), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean Community Score Across Districts")
# trying to understand the most expensive neighborhoods
mean_price_by_district <- aggregate(training_set[, 4:4 ], list(training_set$district), mean)
mean_price_by_district <- data.frame(district_number = mean_price_by_district$Group.1, mean_price = round( mean_price_by_district$x))
p <- ggplot(data = mean_price_by_district, aes(x = district_number, y = mean_price))
p + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=mean_price), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean House Price Across Districts")
# trying to understand the most expensive neighborhoods
mean_community_score_by_district <- aggregate(training_set[, 19:19 ], list(training_set$district), mean)
mean_community_score_by_district <- data.frame(district_number = mean_community_score_by_district$Group.1, community_avg = round( mean_community_score_by_district$x))
p2 <- ggplot(data = mean_community_score_by_district, aes(x = district_number, y = community_avg))
p2 + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=community_avg), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) + +
ggtitle("Distribution of Mean Community Score Across Districts")
p2 + geom_bar(stat="identity", fill = "steelblue") + geom_text(aes(label=community_avg), vjust=1.5, color="white", size=3.5) +
theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +
ggtitle("Distribution of Mean Community Score Across Districts")
# TODO : how are building types distributed across districts?
building_type_by_district <- aggregate(training_set[, 11:11 ], list(training_set$district), )
